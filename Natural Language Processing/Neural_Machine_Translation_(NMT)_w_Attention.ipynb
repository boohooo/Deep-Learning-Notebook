{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation (NMT) w Attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCBI779dc_Il"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from string import digits\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYz7Wdxjc_Kz"
      },
      "source": [
        "path_to_file = \"/content/drive/MyDrive/Colab Notebooks/kor.txt\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAv2E8YEc_M6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "621e5795-4f42-46c6-fe99-f278883d23da"
      },
      "source": [
        "lines = pd.read_table(path_to_file, names=['source', 'target', 'others'])\n",
        "lines = lines[['source', 'target']]\n",
        "lines.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>가.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>안녕.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>뛰어!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Run.</td>\n",
              "      <td>뛰어.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Who?</td>\n",
              "      <td>누구?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  source target\n",
              "0    Go.     가.\n",
              "1    Hi.    안녕.\n",
              "2   Run!    뛰어!\n",
              "3   Run.    뛰어.\n",
              "4   Who?    누구?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8WZairG6alv"
      },
      "source": [
        "np.savetxt('/content/drive/MyDrive/Colab Notebooks/kor2.txt', lines.values, fmt='%s', delimiter=\"\\t\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDwYBuM2645n"
      },
      "source": [
        "path_to_nfile = \"/content/drive/MyDrive/Colab Notebooks/kor2.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Ucnov_c_PG"
      },
      "source": [
        "# Text preprocessing\n",
        "def preprocess_sentence(x):\n",
        "\n",
        "\n",
        "  # Lowercase\n",
        "  x = x.lower()\n",
        "\n",
        "  x = re.sub(\" +\", \" \", x)\n",
        "  x = re.sub(\"'\", '', x)\n",
        "\n",
        "  # Add space between letter and punctuation\n",
        "  x = re.sub(r\"([?.!,¿])\", r\" \\1 \", x)\n",
        "  x = re.sub(r'[\" \"]+', \" \", x)\n",
        "\n",
        "  x = x.strip()\n",
        "\n",
        "  # Add start and end token to sentences\n",
        "  x = '<start> ' + x + ' <end>'\n",
        "  return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjuW-gRd5R3m",
        "outputId": "04a10914-4169-4ac5-c6b6-579e78b7b74e"
      },
      "source": [
        "en_sentence = u\"Do you like cooking?\"\n",
        "kr_sentence = u\"요리 좋아해?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(kr_sentence))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> do you like cooking ? <end>\n",
            "<start> 요리 좋아해 ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPjrBAyFc_RH"
      },
      "source": [
        "# Cleaning text \n",
        "# Add word pairs\n",
        "def create_dataset(path, num_examples):\n",
        "  \n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "  \n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  \n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO9-CdFbc_TS"
      },
      "source": [
        "# En-Kor word pairs | Source-Target\n",
        "sample_size = 50000\n",
        "en, kr = create_dataset(path_to_nfile, sample_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7OpMFJgc_a9"
      },
      "source": [
        "# Tokenizer for source (en)\n",
        "source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "source_tokenizer.fit_on_texts(en)\n",
        "# Sequencing\n",
        "source_tensor = source_tokenizer.texts_to_sequences(en)\n",
        "# Padding\n",
        "source_tensor = tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlNdVPjUc_fZ"
      },
      "source": [
        "# Tokenizer for target (kr)\n",
        "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "target_tokenizer.fit_on_texts(kr)\n",
        "# Sequencing\n",
        "target_tensor = target_tokenizer.texts_to_sequences(kr)\n",
        "# Padding\n",
        "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_RSaH8wc_Zo"
      },
      "source": [
        "# TTS\n",
        "source_train, source_test, target_train, target_test = train_test_split(source_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDaYxgFUc_Vg"
      },
      "source": [
        "# Some parameters\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(source_train)\n",
        "steps = len(source_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "# Vocab size\n",
        "source_vocab_size = len(source_tokenizer.word_index)+1\n",
        "target_vocab_size = len(target_tokenizer.word_index)+1\n",
        "\n",
        "# Create dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((source_train, target_train)).shuffle(BATCH_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "source_batch, target_batch = next(iter(dataset))\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edlvD2uQLAQx"
      },
      "source": [
        "# Encoder\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size= batch_size\n",
        "        self.encoder_units=encoder_units\n",
        "        self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru= tf.keras.layers.GRU(encoder_units, \n",
        "                                      return_sequences=True,\n",
        "                                      return_state=True,\n",
        "                                      recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    def call(self, x, hidden):\n",
        "        #pass the input x to the embedding layer\n",
        "        x= self.embedding(x)\n",
        "        # pass the embedding and the hidden state to GRU\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.encoder_units))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE4q6J0LLATN"
      },
      "source": [
        "encoder = Encoder(source_vocab_size, embedding_dim, units, BATCH_SIZE)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkUoaJ9fLAWF"
      },
      "source": [
        "# Bahdanau Attention Layer\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1= tf.keras.layers.Dense(units)  # Encoder output\n",
        "    self.W2= tf.keras.layers.Dense(units)  # Decoder hidden\n",
        "    self.V= tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, query, values):\n",
        "    #calculate the Attention score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    \n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    \n",
        "      #context_vector \n",
        "    context_vector = attention_weights * values\n",
        "    \n",
        "    #Computes the sum of elements across dimensions of a tensor\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KogzVINKK-yK"
      },
      "source": [
        "# Decoder\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.decoder_units = decoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(decoder_units, \n",
        "                                   return_sequences= True, \n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    # Fully connected layer\n",
        "    self.fc= tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    # attention\n",
        "    self.attention = BahdanauAttention(self.decoder_units)\n",
        "    \n",
        "  def call(self, x, hidden, encoder_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
        "    \n",
        "    # pass output sequnece thru the input layers\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    # concatenate context vector and embedding for output sequence\n",
        "    x = tf.concat([tf.expand_dims( context_vector, 1), x], axis=-1)\n",
        "    \n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "    \n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output= tf.reshape(output, (-1, output.shape[2]))\n",
        "    \n",
        "    # pass the output thru Fc layers\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcHrP1eL9-Ru"
      },
      "source": [
        "decoder = Decoder(target_vocab_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZrXuG8PK-4L"
      },
      "source": [
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# Loss\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):                      \n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GJoFkSyK-7N"
      },
      "source": [
        "# Training\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTHI1sK5K-3K"
      },
      "source": [
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} loss {}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
        "      \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  # if (epoch + 1) % 2 == 0:\n",
        "  #   checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfENIQo7K-1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "ba07d005-8a7e-4a8e-b978-3329bdde5310"
      },
      "source": [
        "#Calculating the max length of the source and target sentences\n",
        "max_target_length= max(len(t) for t in target_tensor)\n",
        "max_source_length= max(len(t) for t in source_tensor)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-21747d791ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Calculating the max length of the source and target sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmax_target_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmax_source_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'target_tensor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI69itnU5WM2"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot= np.zeros((max_target_length, max_source_length))\n",
        "  #preprocess the sentnece\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "  \n",
        "  #convert the sentence to index based on word2index dictionary\n",
        "  inputs= [source_sentence_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  \n",
        "  # pad the sequence \n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_source_length, padding='post')\n",
        "  \n",
        "  #conver to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  \n",
        "  result= ''\n",
        "  \n",
        "  # creating encoder\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  encoder_output, encoder_hidden= encoder(inputs, hidden)\n",
        "  \n",
        "  # creating decoder\n",
        "  decoder_hidden = encoder_hidden\n",
        "  decoder_input = tf.expand_dims([target_sentence_tokenizer.word_index['start_']], 0)\n",
        "    \n",
        "  for t in range(max_target_length):\n",
        "\n",
        "    predictions, decoder_hidden, attention_weights= decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "    \n",
        "    # storing attention weight for plotting it\n",
        "    attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    \n",
        "    prediction_id= tf.argmax(predictions[0]).numpy()\n",
        "    result += target_sentence_tokenizer.index_word[prediction_id] + ' '\n",
        "    \n",
        "    if target_sentence_tokenizer.index_word[prediction_id] == '_end':\n",
        "      return result,sentence, attention_plot\n",
        "    \n",
        "    # predicted id is fed back to as input to the decoder\n",
        "    decoder_input = tf.expand_dims([prediction_id], 0)\n",
        "      \n",
        "  return result,sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq7eEwdP5WQ5"
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax= fig.add_subplot(1,1,1)\n",
        "  ax.matshow(attention, cmap='Greens')\n",
        "  fontdict={'fontsize':10}\n",
        "  \n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8O0LoNL5WUr"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  \n",
        "  print('Input : %s' % (sentence))\n",
        "  print('predicted sentence :{}'.format(result))\n",
        "  \n",
        "  attention_plot= attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOyFv_Yl5WTU"
      },
      "source": [
        "translate(u'I am going to work.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV5URtlG5WPO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}